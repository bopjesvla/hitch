import html
import os
import sys
from jinja2 import Environment, FileSystemLoader

import subprocess

import networkx
import numpy as np
import pandas as pd
import geopandas
import geopandas as gpd
from helpers import get_bearing, haversine_np, root_dir, get_db

dist_dir = os.path.abspath(os.path.join(root_dir, "dist"))
template_dir = os.path.abspath(os.path.join(root_dir, "templates"))

# Set up Jinja2 environment
env = Environment(loader=FileSystemLoader(template_dir))  # Load templates from current directory
template = env.get_template("index_template.html")  # Load template file

os.makedirs(dist_dir, exist_ok=True)

LIGHT = "light" in sys.argv
NEW = "new" in sys.argv

if LIGHT:
    outname = os.path.join(dist_dir, "light.html")
elif NEW:
    outname = os.path.join(dist_dir, "new.html")
else:
    outname = os.path.join(dist_dir, "index.html")

outname_recent = os.path.join(dist_dir, "recent.html")
outname_dups = os.path.join(dist_dir, "recent-dups.html")

points = pd.read_sql(
    sql="select * from points where not banned order by datetime is not null desc, datetime desc",
    con=get_db(),
)

points["user_id"] = points["user_id"].astype(pd.Int64Dtype())

duplicates = pd.read_sql("select * from duplicates where reviewed = accepted", get_db())

# merging and transforming data
dup_rads = duplicates[["from_lon", "from_lat", "to_lon", "to_lat"]].values.T

duplicates["distance"] = haversine_np(*dup_rads)
duplicates["from"] = duplicates[["from_lat", "from_lon"]].apply(tuple, axis=1)
duplicates["to"] = duplicates[["to_lat", "to_lon"]].apply(tuple, axis=1)

duplicates = duplicates[duplicates.distance < 1.25]

dups = networkx.from_pandas_edgelist(duplicates, "from", "to")
islands = networkx.connected_components(dups)

replace_map = {}

for component_id, island in enumerate(islands):
    parents = [node for node in island if node not in duplicates["from"].tolist()]

    if len(parents) == 1:
        for node in island:
            if node != parents[0]:
                replace_map[node] = parents[0]

print("Currently recorded duplicate spots are represented by:", dups)

points[["lat", "lon"]] = points[["lat", "lon"]].apply(lambda x: replace_map.get(tuple(x), x), axis=1, raw=True)

points = geopandas.GeoDataFrame(points, geometry=geopandas.points_from_xy(points.lon, points.lat), crs="EPSG:4326")

service_areas = pd.read_sql("select * from service_areas", get_db())
service_area_geoms = gpd.GeoDataFrame(
    service_areas[["geom_id"]],
    geometry=gpd.GeoSeries.from_wkt(service_areas.geometry_wkt),
    crs="EPSG:4326",
)

points["service_area_id"] = points.sjoin(service_area_geoms, how="left")["geom_id"]

road_islands = pd.read_sql("select * from road_islands", get_db())
road_island_geoms = gpd.GeoDataFrame(
    road_islands[["id"]],
    geometry=gpd.GeoSeries.from_wkt(road_islands.geometry_wkt),
    crs="EPSG:4326",
)

points["road_island_id"] = points.sjoin(road_island_geoms, how="left").drop_duplicates("id_left")["id_right"]


# pseudo-random cluster id based on lat/lon
points["cluster_id"] = (points.lat * 1e10 + points.lon * 1e10).round()

has_road_island = points.road_island_id.notna()
points.loc[has_road_island, "cluster_id"] = points[has_road_island].road_island_id + 1e9

has_service_area = points.service_area_id.notna()
points.loc[has_service_area, "cluster_id"] = points[has_service_area].service_area_id + 5e9

try:
    users = pd.read_sql("select * from user", get_db())
except pd.errors.DatabaseError:
    raise Exception("Run server.py to create the user table") from None

print(f"{len(points)} points currently")

# fix hitchwiki comments
points.loc[points.id.isin(range(1000000, 1040000)), "comment"] = (
    points.loc[points.id.isin(range(1000000, 1040000)), "comment"]
    .str.encode("cp1252", errors="ignore")
    .str.decode("utf-8", errors="ignore")
)

points["datetime"] = pd.to_datetime(points.datetime)
points["ride_datetime"] = pd.to_datetime(points.ride_datetime, errors="coerce")  # handels invalid dates

rads = points[["lon", "lat", "dest_lon", "dest_lat"]].values.T

points["ride_distance"] = haversine_np(*rads)
points["direction"] = get_bearing(*rads)

points.loc[(points.ride_distance < 1), "dest_lat"] = None
points.loc[(points.ride_distance < 1), "dest_lon"] = None
points.loc[(points.ride_distance < 1), "direction"] = None
points.loc[(points.ride_distance < 1), "ride_distance"] = None

rounded_dir = 45 * np.round(points.direction / 45)
points["arrows"] = rounded_dir.replace(
    {
        -90: "â†",
        90: "â†’",
        0: "â†‘",
        180: "â†“",
        -180: "â†“",
        -45: "â†–",
        45: "â†—",
        135: "â†˜",
        -135: "â†™",
    }
)


rating_text = "rating: " + points.rating.astype(int).astype(str) + "/5"
destination_text = (
    ", ride: " + np.round(points.ride_distance).astype(str).str.replace(".0", "", regex=False) + " km " + points.arrows
)


points["wait_text"] = None
has_accurate_wait = ~points.wait.isnull() & ~points.datetime.isnull()
points.loc[has_accurate_wait, "wait_text"] = (
    ", wait: "
    + points.wait[has_accurate_wait].astype(int).astype(str)
    + " min"
    + (" " + points.signal[has_accurate_wait].replace({"ask": "ðŸ’¬", "ask-sign": "ðŸ’¬+ðŸª§", "sign": "ðŸª§", "thumb": "ðŸ‘"})).fillna("")
)


def e(s):
    s2 = s.copy()
    s2.loc[~s2.isnull()] = s2.loc[~s2.isnull()].map(lambda x: html.escape(x).replace("\n", "<br>"))
    return s2


points["extra_text"] = rating_text + points.wait_text.fillna("") + destination_text.fillna("")

comment_nl = points["comment"] + "\n\n"

# show review without comments in the sidebar if they're new; old reviews may be aggregate ratings that don't make sense
comment_nl.loc[(points.datetime.dt.year > 2021) & points.comment.isnull()] = ""

review_submit_datetime = points.datetime.dt.strftime(", %B %Y").fillna("")

points["username"] = pd.merge(
    left=points[["user_id"]],
    right=users[["id", "username"]],
    left_on="user_id",
    right_on="id",
    how="left",
)["username"]
points["hitchhiker"] = points["nickname"].fillna(points["username"])

points["user_link"] = ("<a href='/?user=" + e(points["hitchhiker"]) + "#filters'>" + e(points["hitchhiker"]) + "</a>").fillna(
    "Anonymous"
)

points["text"] = (
    e(comment_nl)
    + "<i>"
    + e(points["extra_text"])
    + "</i><br><br>â€•"
    + points["user_link"]
    + points.ride_datetime.dt.strftime(", %a %d %b %Y, %H:%M").fillna(review_submit_datetime)
)

oldies = points.datetime.dt.year <= 2021
points.loc[oldies, "text"] = (
    e(comment_nl[oldies]) + "â€•" + points.loc[oldies, "user_link"] + points[oldies].datetime.dt.strftime(", %B %Y").fillna("")
)

# has_text = ~points.text.isnull()
# points.loc[has_text, 'text'] = points.loc[has_text, 'text'].map(lambda x: html.escape(x).replace('\n', '<br>'))

groups = points.groupby("cluster_id")

print("After clustering:", len(groups), "Before:", len(points.geometry.drop_duplicates()))

places = groups[["country"]].first()
places["rating"] = groups.rating.mean().round()
places["wait"] = points[~points.wait.isnull()].groupby("cluster_id").wait.mean()
places["ride_distance"] = points[~points.ride_distance.isnull()].groupby("cluster_id").ride_distance.mean()
places["text"] = groups.text.apply(lambda t: "<hr>".join(t.dropna()))
places["review_count"] = groups.size()

# to prevent confusion, only add a review user if their review is listed
places["review_users"] = points.dropna(subset=["text", "hitchhiker"]).groupby("cluster_id").hitchhiker.unique().apply(list)

places["dest_lats"] = points.dropna(subset=["dest_lat", "dest_lon"]).groupby("cluster_id").dest_lat.apply(list)
places["dest_lons"] = points.dropna(subset=["dest_lat", "dest_lon"]).groupby("cluster_id").dest_lon.apply(list)
places["lat"] = groups.lat.mean()
places["lon"] = groups.lon.mean()

if LIGHT:
    places = places[(places.text.str.len() > 0) | ~places.ride_distance.isnull()]
elif NEW:
    places = places[~places.ride_distance.isnull()]

# z-index is rating + 2 * no of reviews + 2 * no of reviews with destination
places["z-index"] = places["rating"] + 2 * places["review_count"] + 2 * places["dest_lats"].str.len().fillna(0)

places.reset_index(inplace=True)
# make sure high-rated are on top
places.sort_values("z-index", inplace=True, ascending=True)

marker_data = places[
    [
        "lat",
        "lon",
        "rating",
        "text",
        "wait",
        "ride_distance",
        "review_users",
        "dest_lats",
        "dest_lons",
    ]
].to_json(orient="values")

try:
    subprocess.run(["npm", "run", "build"], check=True, text=True)
except subprocess.CalledProcessError as e:
    print("DID NOT BUILD JS")

js_output_file = os.path.join(dist_dir, "out.js")

# We embed everything directly into the HTML page so our service worker can't serve inconsistent files
# For example, if we add a new attribute to the spot which is shown in the front-end, but the user only gets the new
# presentation layer, not the new data, the application would break
# Because the HTML file contains everything, this is not a problem

with open(js_output_file, encoding="utf-8") as f:
    hitch_script = f.read()

with open(os.path.join(root_dir, "static", "style.css"), encoding="utf-8") as f:
    hitch_style = f.read()

output = template.render({"hitch_script": hitch_script, "hitch_style": hitch_style, "markers": marker_data})

with open(outname, "w", encoding="utf-8") as f:
    f.write(output)

if not LIGHT:
    recent = points.dropna(subset=["datetime"]).sort_values("datetime", ascending=False).iloc[:1000]
    recent["url"] = "https://hitchmap.com/#" + recent.lat.astype(str) + "," + recent.lon.astype(str)
    recent["text"] = points.comment.fillna("") + " " + points.extra_text.fillna("")
    recent["hitchhiker"] = recent.hitchhiker.str.replace("://", "", regex=False)
    recent["distance"] = recent["ride_distance"].round(1)
    recent["datetime"] = recent["datetime"].astype(str)
    recent["datetime"] += np.where(~recent.ride_datetime.isnull(), " ðŸ•’", "")

    recent[["url", "country", "datetime", "hitchhiker", "rating", "distance", "text"]].to_html(
        outname_recent, render_links=True, index=False
    )

    duplicates["from_url"] = "https://hitchmap.com/#" + duplicates.from_lat.astype(str) + "," + duplicates.from_lon.astype(str)
    duplicates["to_url"] = "https://hitchmap.com/#" + duplicates.to_lat.astype(str) + "," + duplicates.to_lon.astype(str)
    duplicates[["id", "from_url", "to_url", "distance", "reviewed", "accepted"]].to_html(
        outname_dups, render_links=True, index=False
    )
